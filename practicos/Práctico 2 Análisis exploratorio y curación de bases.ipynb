{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM0IWKT72IRevgT9yIcMtxW"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","source":["# Introducción\n","\n","Obtener información sobre el proceso de recolección de datos resulta fundamental para entender nuestra base y poder tomar decisiones justificadas a la hora de la limpieza y curación de la misma. A continuación se comparte un fragmento de la metodología aplicada para la recolección de datos, la cual tiene por objetivo brindar mayor información sobre la base trabajada.\n","\n","### **Descripción del proceso de recolección de datos**\n","\n","La recolección de los tweets se realizó a través de un proceso de scrapping, utilizando la libreria [twscrape](https://github.com/vladkens/twscrape). Para la búsqueda de tweets, se utilizó un filtro de palabras claves relacionadas a obesidad. La recolección de datos tuvo lugar entre enero de 2024 y diciembre de 2024, de forma ininterrumpida. Así, se obtuvo una muestra de 11111 tweets. La recolección se llevó a cabo con un script que utiliza el lenguaje de programación Python, el cual permitió almacenar los datos en MongoDB, una base de datos no relacional. Este trabajo utilizó recursos computacionales del Centro de Cómputos de Alto Desempeño (CCAD) de la Universidad Nacional de Córdoba (https://ccad.unc.edu.ar/), que forman parte del Sistema Nacional de Computación de Alto Desempeño del MinCyT de la República Argentina.\n","\n","#### Selección de palabras clave\n","\n","Para poder identificar aquellos tweets que correspondían a la temática de obesidad se utilizaron palabras clave como filtro de búsqueda. Este filtro al momento de ser utilizado (año 2024) no permite el uso de expresiones regulares (las cuales permiten considerar patrones para hacer coincidir secuencias de caracteres). Dada esta situación, y asumiendo que existen diversas formas coloquiales de escribir en esta red social que pueden contener errores de ortografía, de tipeo, entre otros, se tomó la decisión de sistematizar el proceso de selección de palabras clave para que el mismo sea objetivo y se pierda la menor cantidad de información posible. Además, se consideró que existen otras palabras que son utilizadas por los usuarios/as de Twitter para referirse a la obesidad, justificando la búsqueda de posibles sinónimos. En un comienzo se identificaron las palabras consideradas “semilla”, es decir aquellas que servirían como precursoras del resto de las palabras. Las palabras semilla consideradas fueron las distintas flexiones de la palabra obesidad: “obesidad”, “obeso”, “obesa”, “obesos” y “obesas”. Para reconocer otras palabras claves que estuvieran relacionadas a la temática y que pueden ser utilizadas en el mismo contexto se empleó la técnica de Word Embedding (WE). El WE es una técnica de PLN que utiliza distintos métodos para representar las palabras de un texto mediante vectores de números reales. Las palabras se ubican en el espacio vectorial de acuerdo con la relación y similitud semántica que guardan entre ellas. La distancia entre las mismas se denomina índice de similitud de coseno, el cual mientras más cercano a 1, mayor será la similitud entre las palabras observadas. El WE utilizado fue entrenado previamente con un corpus de tweets de Argentina por investigadores de la Facultad de Matemática, Astronomía, Física y Computación – FaMAF (Peréz y Luque, 2019). Se identificaron aquellas 50 palabras que presentaron un mayor índice de similitud del coseno por cada palabra semilla mencionada. Luego, se seleccionaron como palabras claves sólo aquellas que cumplieran con un criterio de sinonimia, es decir, que puedan ser utilizadas en reemplazo de la palabra semilla y dado el mismo contexto no perdieran el significado semántico. Así, finalmente se obtuvieron un total de 21 palabras clave:          \n","       \n","*'Obesidad', 'obesidaddd', 'gordura', 'sobrepeso', 'gorduras', 'adiposidad', 'Sobrepeso', 'obesito', 'obesoo', 'obesidad', 'obeso', 'obesa', 'gorrrdura', 'obesaa', 'obesita', 'Obesa', 'obesaaa', 'Obesos', 'Obesas', 'obesas', 'obesos'.*\n","\n","#### Geolocalización de los tweets\n","\n","Para asignar un valor de provincia a cada publicación se utilizó la variable \"location\" la cual es la localización expresada por el usuario. Puede ser una ciudad, provincia, país, región, entre otras, lo cual dificulta el proceso de etiquetado. En este trabajo, se observaron las frecuencias de las ciudades mencionadas reconocimiendo simila\n","\n","### **Problemas interesantes a abordar**\n","\n","¿Cuáles son los principales temas de conversación sobre obesidad en Twitter?\n","¿Existen diferencias en el contenido compartido según la provincia? ¿Y según el mes?\n","¿Cómo varía el nivel de interacción (likes, retweets, respuestas) según el tipo de contenido del tweet?\n","¿Se pueden detectar períodos de mayor actividad relacionados con eventos específicos?\n","¿La cantidad de palabras que se utilizan en el tweet se asocia a un sentimiento negativo o positivo, o es indistinto?\n","\n","### **Descripción del dataset**\n","Este repositorio contiene una base de datos con 11111 registros y 13 columnas recopilados de Twitter a partir de palabras clave sobre obesidad. La información fue extraída con el objetivo de analizar conversaciones en torno a la obesidad en Argentina, incluyendo interacciones, fechas, hashtags y datos geográficos. Para identificar la provincia de origen de los usuarios que generaron los tweets, se realizó un proceso de reconocimiento de localidades, ciudades y departamentos más relevantes y frecuentes a partir de la variable \"location\" asociada a cada usuario. Posteriormente, se asignó una provincia a cada tweet con base en esa información. En los casos en que un tweet presenta dos o más provincias identificadas, esto se debe a que el usuario mencionó múltiples lugares en su descripción de ubicación. Esta variable se denominó \"pcia\".\n","\n","Más información disponible [aquí](https://github.com/eugehaluszka/mentoria_2025/tree/main/datasets).\n","\n","## **Práctico 1**\n","\n","En esta primera etapa, se trabajará con el dataset de tweets sobre obesidad para realizar un análisis exploratorio inicial. El objetivo es identificar patrones generales en los datos mediante visualizaciones: distribución de tweets por provincia o mes, volumen de interacciones (likes, retweets, respuestas), y posibles tendencias en la conversación pública. También se podrán contemplar la cantidad de palabras utilizadas en cada tweet, así como los términos más utilizados entre otros análisis descriptivos.\n","Se espera que este práctico permita conocer mejor la estructura de los datos y generar preguntas o hipótesis para análisis posteriores."],"metadata":{"id":"nbAcZH4AzHPc"}},{"cell_type":"markdown","source":["### **Cargando el dataset**"],"metadata":{"id":"bQhnlSK1Bdes"}},{"cell_type":"code","metadata":{"id":"vgbekUHRz24G","executionInfo":{"status":"ok","timestamp":1751377962866,"user_tz":180,"elapsed":1793,"user":{"displayName":"Eugenia Haluszka","userId":"12654016835032146489"}}},"source":["#Importar librerias de interes\n","\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import seaborn as sns\n","import missingno as msno\n","import zipfile\n","%matplotlib inline"],"execution_count":1,"outputs":[]},{"cell_type":"code","source":["#Descargar la base desde el github\n","\n","!wget -O base_obesidad_completa.zip \"https://github.com/eugehaluszka/mentoria_2025/raw/refs/heads/main/datasets/base_obesidad_completa.zip\"\n","with zipfile.ZipFile(\"base_obesidad_completa.zip\", \"r\") as z:\n","    z.extractall()"],"metadata":{"id":"dh9R5G76JMJ2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1751377963702,"user_tz":180,"elapsed":828,"user":{"displayName":"Eugenia Haluszka","userId":"12654016835032146489"}},"outputId":"a72f8adb-111f-4858-bc00-ef27224668d2"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["--2025-07-01 13:52:42--  https://github.com/eugehaluszka/mentoria_2025/raw/refs/heads/main/datasets/base_obesidad_completa.zip\n","Resolving github.com (github.com)... 140.82.112.3\n","Connecting to github.com (github.com)|140.82.112.3|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://raw.githubusercontent.com/eugehaluszka/mentoria_2025/refs/heads/main/datasets/base_obesidad_completa.zip [following]\n","--2025-07-01 13:52:43--  https://raw.githubusercontent.com/eugehaluszka/mentoria_2025/refs/heads/main/datasets/base_obesidad_completa.zip\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 1043853 (1019K) [application/zip]\n","Saving to: ‘base_obesidad_completa.zip’\n","\n","base_obesidad_compl 100%[===================>]   1019K  --.-KB/s    in 0.04s   \n","\n","2025-07-01 13:52:43 (28.4 MB/s) - ‘base_obesidad_completa.zip’ saved [1043853/1043853]\n","\n"]}]},{"cell_type":"code","source":["#Abrir la base de datos\n","df = pd.read_csv(\"base_obesidad_completa.csv\", encoding=\"utf-8-sig\")\n","df_original = df.copy() #se genera copia\n","df.head(3)"],"metadata":{"id":"Z-xKtjaK9SYD","colab":{"base_uri":"https://localhost:8080/","height":230},"executionInfo":{"status":"ok","timestamp":1751377964021,"user_tz":180,"elapsed":316,"user":{"displayName":"Eugenia Haluszka","userId":"12654016835032146489"}},"outputId":"f62f0b50-ffd4-435c-bb30-99af88d431f6"},"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                id_str                 date  \\\n","0  1747300752475730129  2024-01-16 16:52:17   \n","1  1747297783743197385  2024-01-16 16:40:29   \n","2  1747295025363341318  2024-01-16 16:29:31   \n","\n","                                          rawContent  replyCount  \\\n","0  Boluda ahí dice que yo debería pesar 43kg eso ...           1   \n","1  Jugador de +30 años, vendehumo, con tendencias...           0   \n","2  yo, re pesada mal: ✨EL UNIVERSO CONSPIRA A MI ...           0   \n","\n","   retweetCount  likeCount  quoteCount       conversationId hashtags  \\\n","0             0        1.0           0  1747290800474476846       []   \n","1             0        1.0           0  1747268216882102581       []   \n","2             0        0.0           0  1747295025363341318       []   \n","\n","   viewCount              pcia  mes  user_id  \n","0       18.0     ['argentina']    1        0  \n","1       21.0  ['buenos aires']    1        1  \n","2       54.0  ['buenos aires']    1        2  "],"text/html":["\n","  <div id=\"df-a5d471cd-6cd3-4d09-b388-562881e40630\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id_str</th>\n","      <th>date</th>\n","      <th>rawContent</th>\n","      <th>replyCount</th>\n","      <th>retweetCount</th>\n","      <th>likeCount</th>\n","      <th>quoteCount</th>\n","      <th>conversationId</th>\n","      <th>hashtags</th>\n","      <th>viewCount</th>\n","      <th>pcia</th>\n","      <th>mes</th>\n","      <th>user_id</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1747300752475730129</td>\n","      <td>2024-01-16 16:52:17</td>\n","      <td>Boluda ahí dice que yo debería pesar 43kg eso ...</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1.0</td>\n","      <td>0</td>\n","      <td>1747290800474476846</td>\n","      <td>[]</td>\n","      <td>18.0</td>\n","      <td>['argentina']</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1747297783743197385</td>\n","      <td>2024-01-16 16:40:29</td>\n","      <td>Jugador de +30 años, vendehumo, con tendencias...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1.0</td>\n","      <td>0</td>\n","      <td>1747268216882102581</td>\n","      <td>[]</td>\n","      <td>21.0</td>\n","      <td>['buenos aires']</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1747295025363341318</td>\n","      <td>2024-01-16 16:29:31</td>\n","      <td>yo, re pesada mal: ✨EL UNIVERSO CONSPIRA A MI ...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>1747295025363341318</td>\n","      <td>[]</td>\n","      <td>54.0</td>\n","      <td>['buenos aires']</td>\n","      <td>1</td>\n","      <td>2</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a5d471cd-6cd3-4d09-b388-562881e40630')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-a5d471cd-6cd3-4d09-b388-562881e40630 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-a5d471cd-6cd3-4d09-b388-562881e40630');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","    <div id=\"df-679eefcd-d65c-44b3-8beb-730d27c77967\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-679eefcd-d65c-44b3-8beb-730d27c77967')\"\n","                title=\"Suggest charts\"\n","                style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","      <script>\n","        async function quickchart(key) {\n","          const quickchartButtonEl =\n","            document.querySelector('#' + key + ' button');\n","          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","          quickchartButtonEl.classList.add('colab-df-spinner');\n","          try {\n","            const charts = await google.colab.kernel.invokeFunction(\n","                'suggestCharts', [key], {});\n","          } catch (error) {\n","            console.error('Error during call to suggestCharts:', error);\n","          }\n","          quickchartButtonEl.classList.remove('colab-df-spinner');\n","          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","        }\n","        (() => {\n","          let quickchartButtonEl =\n","            document.querySelector('#df-679eefcd-d65c-44b3-8beb-730d27c77967 button');\n","          quickchartButtonEl.style.display =\n","            google.colab.kernel.accessAllowed ? 'block' : 'none';\n","        })();\n","      </script>\n","    </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"df","summary":"{\n  \"name\": \"df\",\n  \"rows\": 11111,\n  \"fields\": [\n    {\n      \"column\": \"id_str\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 37019124389829968,\n        \"min\": 1742551559316107646,\n        \"max\": 1870045511706620107,\n        \"num_unique_values\": 11111,\n        \"samples\": [\n          1838634299945341260,\n          1795661430106243169,\n          1785136526780699125\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"date\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 11102,\n        \"samples\": [\n          \"2024-03-12 16:19:51\",\n          \"2024-07-13 01:08:25\",\n          \"2024-03-19 17:36:54\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"rawContent\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10974,\n        \"samples\": [\n          \"con los gordos de mi novio y mi amigo que me quieren hacer comer hamburguesa todos los partidos voy a terminar obesa, igual los amo no pasa nada\\ud83d\\udc98\",\n          \"fabra es lo mas cercano a obesidad m\\u00f3rbida que mis ojos pudieron presenciar\",\n          \"Est\\u00e1 bien que la obesidad no es saludable , pero a quien quer\\u00e9s descansar pesando 10 Kg? Jajaja\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"replyCount\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 707,\n        \"min\": 0,\n        \"max\": 49463,\n        \"num_unique_values\": 617,\n        \"samples\": [\n          360,\n          2371,\n          1987\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"retweetCount\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1852,\n        \"min\": 0,\n        \"max\": 130263,\n        \"num_unique_values\": 733,\n        \"samples\": [\n          42,\n          257,\n          29\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"likeCount\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 17327.26728160041,\n        \"min\": 0.0,\n        \"max\": 878342.0,\n        \"num_unique_values\": 1150,\n        \"samples\": [\n          4040.0,\n          10125.0,\n          30328.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"quoteCount\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 669,\n        \"min\": 0,\n        \"max\": 31877,\n        \"num_unique_values\": 592,\n        \"samples\": [\n          3941,\n          970,\n          45\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"conversationId\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 39866244603820832,\n        \"min\": 587795061225697280,\n        \"max\": 1870029828369330339,\n        \"num_unique_values\": 9794,\n        \"samples\": [\n          1805719333101879543,\n          1823811585162535224,\n          1814728489209069619\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"hashtags\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 311,\n        \"samples\": [\n          \"['LeydeEtiquetado']\",\n          \"['GranHermano', 'gh2024', 'GH2023']\",\n          \"['Polar', 'ChicaPolar']\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"viewCount\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2692943.6657111626,\n        \"min\": 1.0,\n        \"max\": 154426838.0,\n        \"num_unique_values\": 2368,\n        \"samples\": [\n          696.0,\n          8011.0,\n          1082.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"pcia\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 27,\n        \"samples\": [\n          \"['la pampa']\",\n          \"['tierra del fuego']\",\n          \"['chubut']\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mes\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3,\n        \"min\": 1,\n        \"max\": 12,\n        \"num_unique_values\": 12,\n        \"samples\": [\n          10,\n          11,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"user_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 249,\n        \"min\": 0,\n        \"max\": 1013,\n        \"num_unique_values\": 1014,\n        \"samples\": [\n          752,\n          519,\n          210\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":3}]},{"cell_type":"markdown","source":["### Procesamiento y curación del dataset, para su posterior aplicación de machine learning\n","\n","Al finalizar este práctico 2 obtendrán una base de datos lista para usar en el práctico 3. Es por eso que deben entregar la notebook con las resoluciones y la base de datos alojada en un repositorio (puede ser el mismo de la mentoria, me pasan sus usuarios y les doy permiso a editar el repo), o alojada en un repositorio de uds mismos.\n","\n","### **Limpieza de la base de datos**\n","\n","#### Ejercicio 1.\n","A partir de la información brindada sobre el proceso de recolección y construcción de la base de datos se propone:\n","\n","- a. Eliminar de la base aquellos tweets que, según lo trabajado en el Práctico 1, fueron considerados como irrelevantes para el análisis de esta temática o que podrían generar ruido en las interpretaciones. Este proceso es fundamental para ambos grupos e implica una serie de pasos cíclicos o repetitivos, hasta que la base de datos esté lista para el análisis. Por ello, en este ejercicio se espera que desarrollen y describan el proceso de limpieza de la base de manera detallada, fundamentando adecuadamente cada una de las decisiones tomadas.\n","- b. Ejecutar nuevamente al menos tres de los gráficos descriptivos vistos en el práctico 1, que el grupo considere más relevantes, con el objetivo de observar si la distribución de las variables se ha visto significativamente modificada a partir de los cambios realizados en la base de datos. Se pueden incluir más gráficos si así lo desean.\n"],"metadata":{"id":"xi58GnHGC2-l"}},{"cell_type":"markdown","source":["### **Limpieza del cuerpo de texto**\n","\n","#### Ejercicio 2\n","\n","Para analizar el contenido de tipo texto es importante eliminar todo el ruido que puede dificultar la interpretación del mismo. En este punto será importante conocer sobre expresiones regulares y cómo funcionan. Pueden probar en [regexr](https://regexr.com/) o en [regex101](https://regex101.com/)\n","\n","- a. Se deberá generar una nueva columna en la base que considere el cuerpo de texto \"limpio\". Para ello deberán eliminar del cuerpo de texto todo lo que consideran como ruido para su análisis posterior, justificando el por qué pueden considerarlo ruido o no.\n","Usualmente se suelen eliminar símbolos o caracteres extraños, enlaces, nombres de usuarios, **emojis**, hashtags, entre otros. ¿Ustedes que decisiones tomarían?\n"],"metadata":{"id":"-xbL48IZw8s_"}},{"cell_type":"markdown","source":["### Creación y transformación de variables\n","\n","Para conocer en mayor profundidad el conjunto de datos de texto (corpus) con el que estamos trabajando, es importante aplicar diversas técnicas básicas de procesamiento del lenguaje natural (NLP), las cuales pueden aportar información valiosa. En el Práctico 1, identificar las palabras más frecuentes resultó útil; sin embargo, muchas otras palabras que describen con mayor precisión el contenido de las publicaciones pueden tener baja frecuencia o pasar desapercibidas en un análisis superficial.\n","\n","Por esta razón, es habitual aplicar técnicas como la lematización, que permiten unificar distintas formas flexionadas de una misma palabra (por ejemplo, singular/plural o masculino/femenino), facilitando una representación más coherente del contenido textual.\n","\n","Además, al aplicar el análisis gramatical propio del NLP, podemos reconocer las categorías gramaticales presentes en los textos, como sustantivos, adjetivos y verbos, entre otras. Esto puede enriquecer la comprensión del corpus y abrir nuevas posibilidades de análisis.\n","\n","A partir de estas ideas, se proponen los siguientes ejercicios:\n","\n","#### Ejercicio 3: Exploración básica con NLP\n","\n","- a. Usar una librería de procesamiento de lenguaje natural que sea compatible con corpus en español para procesar el texto de los tweets. Deberán generar una nueva variable en la base de datos que contenga únicamente los términos pertenecientes a una categoría gramatical de su elección. Por ejemplo: una columna llamada \"sustantivos\", que incluya todos los sustantivos reconocidos en cada tweet.\n","\n","  Es importante seleccionar y justificar:  la librería utilizada, y el modelo de lenguaje elegido (por ejemplo, es_core_news_sm en spaCy).\n","\n","*Deberán decidir si aplican el procesamiento NLP sobre el texto limpio o sobre el texto crudo del tweet, y explicar las razones de esa elección.*\n","\n","- b. Aplicar lematización a los términos reconocidos en la categoría gramatical seleccionada, generando una nueva variable con los términos lematizados. Observar y comparar los términos más frecuentes en esta categoría, tanto en su forma lematizada como sin lematizar. Finalmente, reflexionar sobre cuál de las dos representaciones elegirían para describir el dataset y justificar la elección.\n","- c. Discutir cómo podría aprovecharse esta información en tareas como análisis de sentimientos, detección de tópicos o predicción de engagement.\n","\n","#### Ejercicio 4: Transformar variables\n","\n","- a. Identificar columnas que necesiten transformación para ser usadas en modelos de ML (por ejemplo, fechas, categorías, frecuencias absolutas, etc.). *En este punto se espera ver el uso de técnicas de transformación de datos en al menos dos variables. Esto debe realizarse más allá de si será la variable utilizada a futuro*\n","- b. Generar nuevas variables a partir de la columna de fecha (por ejemplo, día de la semana, mes, hora).\n","- c. Convertir variables categóricas en variables numéricas (por ejemplo, usando codificación one-hot o label encoding).\n"],"metadata":{"id":"bAeb1rGV1EFs"}},{"cell_type":"markdown","source":["### Selección de variables relevantes\n","\n","Cuán aplicamos técnicas de ML precisamos reconocer qué variables serán incluídas en nuestro modelo, cómo es la influencia que las mismas tienen sobre nuestra variable respuesta, y si deberíamos incluirlas o no, y cómo. En este caso, la problemática que se presenta es poder identificar qué variables influyen en los patrones de interacción de las publicaciones, específicamente en la cantidad de likes que puede tener un posteo. Para ello se solicitan las siguientes tareas.\n","\n","#### Ejercicio 5\n","\n","a. Identificar qué variables creen que podrían ser relevantes para el problema que abordarán con ML. Mencionar qué otras variables sería interesantes incorporar y que no están disponibles en el dataset proporcionado o que estén disponibles en fuentes externas.\n","b. Aplicar distintar pruebas estadísticas para conocer la influencia/relación que presentan con la variable respuesta mencionada (likes). Interpretar los resultados y mencionar qué variables se pueden incluir y cuales eliminar.\n","c. (**Opcional**) Aplicar técnicas automáticas de selección de características (por ejemplo, usando feature importance, chi2, mutual info, etc.)."],"metadata":{"id":"jSP_xutGk0mf"}},{"cell_type":"markdown","source":["### Ejercicio final\n","\n","* ¿Qué técnicas utilizadas en el práctico les parecieron más interesantes?\n","* Para el próximo práctico 3 deberán aplicar su modelo de ML, para ello es importante realizar una vectorización del texto de los tweets (convertir el texto a números). Algunas técnicas más utilizadas pueden ser TF-IDF, countvectorizer, bag of words, entre otras. Es importante que hagan una revisión/búsqueda sobre cuales ténicas de vectorización utilizar, ventajas y desventajas, y cuál se acomoda mejor a su problemática y corpus.\n","\n","\n","La fecha de entrega del práctico es para el día 21/7, durante julio estaré trabajando (no tengo vacaciones) así que porfavor avisenme si necesitan que nos juntemos (aunque sea con una persona del grupo) para resolver cualquier duda o consulta."],"metadata":{"id":"k7y051dE13SM"}},{"cell_type":"code","source":[],"metadata":{"id":"45Qy_zSOSLg7"},"execution_count":null,"outputs":[]}]}
